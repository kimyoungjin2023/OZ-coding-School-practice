{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 딥러닝 기초: 활성화 함수 (Activation Functions)\n",
    "\n",
    "## 개요\n",
    "활성화 함수는 신경망에 비선형성을 추가하여 복잡한 패턴을 학습할 수 있게 해줍니다.\n",
    "이 실습에서는 Sigmoid, ReLU, Softmax 등 주요 활성화 함수를 구현하고 시각화합니다.\n",
    "\n",
    "## 학습 목표\n",
    "1. 활성화 함수의 필요성 이해\n",
    "2. Sigmoid, ReLU, Softmax 함수 구현 및 시각화\n",
    "3. 각 활성화 함수의 특징과 사용 시나리오 파악\n",
    "4. Gradient Vanishing 문제 이해\n",
    "\n",
    "## 핵심 단계\n",
    "- Step 1: 비선형성의 필요성\n",
    "- Step 2: Sigmoid 함수\n",
    "- Step 3: ReLU 함수\n",
    "- Step 4: Softmax 함수\n",
    "- Step 5: 활성화 함수 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 비선형성의 필요성\n",
    "\n",
    "활성화 함수가 없으면 아무리 층을 많이 쌓아도 결과는 선형 함수입니다.\n",
    "\n",
    "예시:\n",
    "- 층 1: $y = 2x + 1$\n",
    "- 층 2: $y = 3x + 2$\n",
    "- 합치면: $y = 6x + 5$ → 그냥 직선!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 함수의 합성은 여전히 선형\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# 층 1: y = 2x + 1\n",
    "# 층 2: y = 3x + 2\n",
    "# 합성: y = 3(2x + 1) + 2 = 6x + 5\n",
    "\n",
    "layer1 = 2 * x + 1\n",
    "layer2 = 3 * layer1 + 2  # 층 1의 출력을 입력으로\n",
    "combined = 6 * x + 5     # 직접 계산한 결과\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, layer2, 'b-', linewidth=2, label='2 Layers Stacked')\n",
    "plt.plot(x, combined, 'r--', linewidth=2, label='Single Linear (6x + 5)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear functions stacked = Still Linear!')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"두 선형 층을 쌓아도 결과는 단순한 선형 함수입니다.\")\n",
    "print(\"비선형 활성화 함수가 필요한 이유입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Sigmoid 함수\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**특징:**\n",
    "- 출력 범위: 0 ~ 1\n",
    "- 확률로 해석 가능\n",
    "- 이진 분류에 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid 함수\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Sigmoid의 미분 (도함수)\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid 함수 시각화\n",
    "z = np.linspace(-10, 10, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sigmoid 함수\n",
    "ax1 = axes[0]\n",
    "ax1.plot(z, sigmoid(z), 'b-', linewidth=2)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('z')\n",
    "ax1.set_ylabel('sigmoid(z)')\n",
    "ax1.set_title('Sigmoid Function')\n",
    "ax1.set_ylim(-0.1, 1.1)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Sigmoid 도함수\n",
    "ax2 = axes[1]\n",
    "ax2.plot(z, sigmoid_derivative(z), 'r-', linewidth=2)\n",
    "ax2.axhline(y=0.25, color='gray', linestyle='--', alpha=0.5, label='max = 0.25')\n",
    "ax2.set_xlabel('z')\n",
    "ax2.set_ylabel(\"sigmoid'(z)\")\n",
    "ax2.set_title('Sigmoid Derivative (Gradient)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid 함수의 특징:\")\n",
    "print(f\"  sigmoid(-10) = {sigmoid(-10):.6f} (거의 0)\")\n",
    "print(f\"  sigmoid(0) = {sigmoid(0):.1f} (정확히 0.5)\")\n",
    "print(f\"  sigmoid(10) = {sigmoid(10):.6f} (거의 1)\")\n",
    "print(f\"\\n미분값의 최대: {np.max(sigmoid_derivative(z)):.4f} (z=0일 때)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ReLU 함수 (Rectified Linear Unit)\n",
    "\n",
    "$$ReLU(z) = max(0, z)$$\n",
    "\n",
    "**특징:**\n",
    "- 출력 범위: 0 ~ ∞\n",
    "- 계산이 매우 빠름\n",
    "- Gradient Vanishing 문제 완화\n",
    "- 은닉층에서 가장 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"ReLU 함수\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"ReLU의 미분\"\"\"\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU 함수\"\"\"\n",
    "    return np.where(z > 0, z, alpha * z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 함수 시각화\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "ax1 = axes[0]\n",
    "ax1.plot(z, relu(z), 'b-', linewidth=2)\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('z')\n",
    "ax1.set_ylabel('ReLU(z)')\n",
    "ax1.set_title('ReLU Function')\n",
    "ax1.grid(True)\n",
    "\n",
    "# ReLU 도함수\n",
    "ax2 = axes[1]\n",
    "ax2.plot(z, relu_derivative(z), 'r-', linewidth=2)\n",
    "ax2.set_xlabel('z')\n",
    "ax2.set_ylabel(\"ReLU'(z)\")\n",
    "ax2.set_title('ReLU Derivative')\n",
    "ax2.set_ylim(-0.1, 1.5)\n",
    "ax2.grid(True)\n",
    "\n",
    "# Leaky ReLU\n",
    "ax3 = axes[2]\n",
    "ax3.plot(z, leaky_relu(z, 0.1), 'g-', linewidth=2, label='Leaky ReLU (alpha=0.1)')\n",
    "ax3.plot(z, relu(z), 'b--', linewidth=1, alpha=0.7, label='ReLU')\n",
    "ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('z')\n",
    "ax3.set_ylabel('Leaky ReLU(z)')\n",
    "ax3.set_title('Leaky ReLU (Dead Neuron Problem Solution)')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ReLU 함수의 특징:\")\n",
    "print(f\"  ReLU(-5) = {relu(-5)}\")\n",
    "print(f\"  ReLU(0) = {relu(0)}\")\n",
    "print(f\"  ReLU(3) = {relu(3)}\")\n",
    "print(f\"  ReLU(100) = {relu(100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Softmax 함수\n",
    "\n",
    "$$Softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "**특징:**\n",
    "- 출력 범위: 0 ~ 1 (합 = 1)\n",
    "- 확률 분포로 해석 가능\n",
    "- 다중 분류 문제에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Softmax 함수 (수치 안정성 고려)\"\"\"\n",
    "    z = np.array(z)\n",
    "    # Overflow 방지를 위해 최댓값을 빼줌\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / np.sum(exp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 예제\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Softmax 계산 과정:\")\n",
    "print(f\"입력 (logits): {logits}\")\n",
    "print(f\"\\n1. 지수 계산:\")\n",
    "for i, z in enumerate(logits):\n",
    "    print(f\"   e^{z} = {np.exp(z):.4f}\")\n",
    "print(f\"\\n2. 합계: {np.sum(np.exp(logits)):.4f}\")\n",
    "print(f\"\\n3. 정규화 결과:\")\n",
    "for i, p in enumerate(probabilities):\n",
    "    print(f\"   Class {i}: {p:.4f} ({p*100:.1f}%)\")\n",
    "print(f\"\\n확률의 합: {np.sum(probabilities):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬스케어 예제: X-ray 진단\n",
    "# 모델 출력 (logits)\n",
    "xray_logits = np.array([3.5, 1.2, 0.5])  # 폐렴, 정상, 기타\n",
    "xray_probs = softmax(xray_logits)\n",
    "\n",
    "classes = ['Pneumonia', 'Normal', 'Other']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(classes, xray_probs * 100, color=['red', 'green', 'blue'], alpha=0.7)\n",
    "plt.ylabel('Probability (%)')\n",
    "plt.title('X-ray Diagnosis: Softmax Output')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# 막대 위에 값 표시\n",
    "for bar, prob in zip(bars, xray_probs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "             f'{prob*100:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"진단 결과: {classes[np.argmax(xray_probs)]} ({xray_probs[np.argmax(xray_probs)]*100:.1f}% 확률)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 활성화 함수 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    \"\"\"Tanh 함수\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "# 모든 활성화 함수 비교\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(z, sigmoid(z), 'b-', linewidth=2, label='Sigmoid (0~1)')\n",
    "plt.plot(z, tanh(z), 'g-', linewidth=2, label='Tanh (-1~1)')\n",
    "plt.plot(z, relu(z), 'r-', linewidth=2, label='ReLU (0~inf)')\n",
    "plt.plot(z, leaky_relu(z, 0.1), 'm--', linewidth=2, label='Leaky ReLU')\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('f(z)')\n",
    "plt.title('Activation Functions Comparison')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-2, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 선택 가이드\n",
    "print(\"=\"*60)\n",
    "print(\"활성화 함수 선택 가이드\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"| 함수      | 출력 범위   | 주요 용도           | 장점            | 단점           |\")\n",
    "print(\"|-----------|-------------|---------------------|-----------------|----------------|\")\n",
    "print(\"| Sigmoid   | 0 ~ 1       | 이진 분류 출력층    | 확률 해석 가능  | Gradient 소멸  |\")\n",
    "print(\"| Tanh      | -1 ~ 1      | RNN 은닉층          | 0 중심          | Gradient 소멸  |\")\n",
    "print(\"| ReLU      | 0 ~ inf     | 은닉층 (기본 선택)  | 빠름, 안정적    | Dead neuron    |\")\n",
    "print(\"| Softmax   | 0 ~ 1 (합=1)| 다중 분류 출력층    | 확률 분포       | 계산 복잡      |\")\n",
    "print()\n",
    "print(\"실전 가이드:\")\n",
    "print(\"  1. 은닉층: 우선 ReLU 사용 (기본 선택)\")\n",
    "print(\"  2. 이진 분류 출력: Sigmoid (예: 암 유무)\")\n",
    "print(\"  3. 다중 분류 출력: Softmax (예: 10가지 질병 중 하나)\")\n",
    "print(\"  4. 회귀 출력: 활성화 함수 없음 (Linear)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Gradient Vanishing 문제\n",
    "\n",
    "Sigmoid의 미분값은 최대 0.25입니다. 층이 깊어질수록 기울기가 기하급수적으로 작아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Vanishing 시뮬레이션\n",
    "layers = range(1, 21)\n",
    "sigmoid_gradients = [0.25 ** n for n in layers]  # 최대 기울기 가정\n",
    "relu_gradients = [1 ** n for n in layers]        # ReLU: 양수 영역에서 1\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(layers, sigmoid_gradients, 'b-o', label='Sigmoid (max gradient = 0.25)')\n",
    "plt.semilogy(layers, relu_gradients, 'r-s', label='ReLU (gradient = 1)')\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Gradient (log scale)')\n",
    "plt.title('Gradient Vanishing Problem')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid를 사용한 경우 각 층의 기울기:\")\n",
    "for n in [1, 5, 10, 15, 20]:\n",
    "    grad = 0.25 ** n\n",
    "    print(f\"  {n}층: {grad:.2e}\")\n",
    "\n",
    "print(\"\\n결론: 깊은 네트워크에서는 ReLU를 사용해야 학습이 가능합니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 실습에서 배운 내용:\n",
    "\n",
    "1. **활성화 함수의 필요성**: 비선형성을 추가하여 복잡한 패턴 학습\n",
    "2. **Sigmoid**: 0~1 출력, 확률 해석 가능, 이진 분류에 사용\n",
    "3. **ReLU**: 계산 빠름, Gradient Vanishing 완화, 은닉층 기본 선택\n",
    "4. **Softmax**: 확률 분포 출력, 다중 분류에 사용\n",
    "5. **Gradient Vanishing**: 깊은 네트워크에서 Sigmoid의 문제, ReLU로 해결"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
